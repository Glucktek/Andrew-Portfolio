[["Map",1,2,9,10,24,25,223,224,582,583,592,593],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.8.1","content-config-digest","3d093af74060348c","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://andrew-gluck.com\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":\"127.0.0.1\",\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{\"/admin\":\"/keystatic\"},\"image\":{\"endpoint\":{\"route\":\"/_image\",\"entrypoint\":\"astro/assets/endpoint/node\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"experimentalDefaultStyles\":true},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"css-variables\",\"themes\":{},\"wrap\":true,\"transformers\":[]},\"remarkPlugins\":[null],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"i18n\":{\"defaultLocale\":\"en\",\"locales\":[\"en\"],\"routing\":{\"prefixDefaultLocale\":false,\"redirectToDefaultLocale\":true,\"fallbackType\":\"redirect\"}},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false},\"legacy\":{\"collections\":false},\"session\":{\"driver\":\"fs-lite\",\"options\":{\"base\":\"/home/agluck/projects/glucktek/Andrew-Portfolio/node_modules/.astro/sessions\"}}}","authors",["Map",11,12],"main-author",{"id":11,"data":13,"filePath":19,"assetImports":20,"digest":22,"deferredRender":23},{"name":14,"avatar":15,"about":16,"email":17,"authorLink":18},"Andrew Gluck","__ASTRO_IMAGE_../main-author/avatar.jpeg","Helping you achieve your goals with the power of technology","andrew.gluck@glucktek.com","https://glucktek.com","src/data/authors/main-author/index.mdx",[21],"../main-author/avatar.jpeg","9e8b5fcda6c2fc89",true,"projects",["Map",26,27,58,59,88,89,115,116,146,147,171,172,200,201],"en/argocd-hub-spoke-automation",{"id":26,"data":28,"body":53,"filePath":54,"assetImports":55,"digest":57,"deferredRender":23},{"title":29,"description":30,"image":31,"technologies":32,"completionDate":42,"keyFeatures":43,"order":51,"draft":52},"ArgoCD Automated for Hybrid Kubernetes Platform","End-to-end automated GitOps platform that deploys and configures ArgoCD across a hybrid on‑prem and Amazon EKS estate, using Ansible, Helm, Terraform, and the ArgoCD API to bootstrap a central hub, register spoke clusters, and continuously deliver applications with an app-of-apps pattern from GitLab.","__ASTRO_IMAGE_./image.png",[33,34,35,36,37,38,39,40,41],"Kubernetes","ArgoCD","Ansible","Helm","Terraform","GitLab CI/CD","GitOps","Amazon EKS","On‑prem",["Date","2025-06-30T00:00:00.000Z"],[44,45,46,47,48,49,50],"Automated ArgoCD installation and bootstrap via Ansible and Helm","Central \"hub\" ArgoCD on a core management cluster with programmatic bootstrap","Spoke ArgoCD instances on environment clusters (Dev, QA, UAT, Prod) auto-joined","Cluster registration and project/app creation via ArgoCD API and Terraform","App-of-apps pattern sourcing manifests from GitLab repos","Policy, RBAC, and secret management wired at bootstrap time","Works across on‑prem and EKS with consistent workflows",1,false,"### Problem\n\nI needed a reliable way to deliver applications into multiple Kubernetes environments that live across data center and cloud. Manual setup did not scale and produced configuration drift. I wanted each new cluster to become GitOps-enabled automatically, and I wanted one place to observe and drive change.\n\n### Approach\n\nI built a hub-and-spoke GitOps platform centered on ArgoCD. The core (hub) ArgoCD runs on a management cluster. As I provision environment clusters like Dev, QA, UAT, and Prod, the system deploys a local ArgoCD (spoke) and wires it to the hub’s GitOps workflow.\n\n- Bootstrap: Ansible installs ArgoCD on the hub using Helm charts. It applies initial Projects, RBAC, and repository credentials using the ArgoCD API.\n- Cluster onboarding: When a new cluster comes online (on‑prem or EKS), Terraform registers the cluster context and secrets, and Ansible deploys the spoke ArgoCD with environment overlays.\n- App-of-apps: I keep a core infrastructure repo that defines platform services (ingress, cert-manager, metrics, logging, external-dns, secrets integration). A root Application in the hub points to this repo and fans out to component Applications per cluster and per environment.\n- Application delivery: Product teams push changes to GitLab. The hub reconciles desired state and either drives deployment directly to spokes or delegates via app-of-apps in the spoke. Either way, Git is the single source of truth.\n\n### Implementation details\n\n- API automation: I use the ArgoCD API to create Projects, repo connections, cluster secrets, and Applications. This avoids click-ops and ensures reproducibility.\n- Terraform glue: Modules encapsulate cluster registration and ArgoCD objects. Outputs feed Ansible inventories so that bootstrap and configuration stay in lockstep.\n- Secrets and SSO: Secrets are mounted via external secret operators where available. SSO is configured centrally so developer access is consistent across clusters.\n- Safe rollouts: Progressive delivery is handled through Argo Rollouts where applicable, and sync windows protect sensitive environments.\n- Multi-tenancy: Projects and RBAC scopes keep app teams isolated, while shared platform services remain centrally managed.\n\n### Hub and spoke behavior\n\n- Hub: Hosts the root app-of-apps for core services and environment catalogs, provides centralized visibility, and drives policy.\n- Spokes: Run a minimal ArgoCD installation tuned to each environment. They reconcile their own app-of-apps definitions and report status back through the hub dashboards.\n\n### Results\n\n- New clusters become GitOps-ready in minutes with consistent guardrails.\n- Platform services deploy automatically and stay in sync across on‑prem and EKS.\n- Product applications promote cleanly across Dev, QA, UAT, and Prod by merging to Git branches and directories.\n- Operational noise drops because ArgoCD continuously detects and remediates drift.\n\n### Why this project stands out to me\n\nThis is the synthesis of platform automation and practical GitOps. I like how the pieces fit together: Terraform handles registration; Ansible and Helm handle installation; the ArgoCD API provides a clear contract for Projects, clusters, and Applications; and the app-of-apps pattern keeps everything readable in Git. It lets me scale from a single cluster to many without changing the workflow, and it keeps humans out of the critical path while still keeping everything transparent in version control.","src/data/projects/en/argocd-hub-spoke-automation/index.mdx",[56],"./image.png","95d1890386b4b6f3","en/home-lab-production-workflow",{"id":58,"data":60,"body":83,"filePath":84,"assetImports":85,"digest":87,"deferredRender":23},{"title":61,"description":62,"image":63,"technologies":64,"completionDate":74,"keyFeatures":75,"order":82,"draft":52},"Production Homelab (Part 1 - Pi-Cluster and Proxmox)","Production-ready homelab: Proxmox + Portainer for containers, a 5-node Raspberry Pi Kubernetes cluster (Traefik + Longhorn) managed by Flux and migrating to Pulumi’s Kubernetes Operator, Cloudflared ingress, segmented networking, and layered backup/recovery for low MTTR.","__ASTRO_IMAGE_../home-lab-production-workflow/image.png",[65,66,67,68,69,70,71,72,33,73],"Proxmox VE","Portainer","Docker","Cloudflare Tunnels","VLAN Segmentation","IPS/Firewall","ZFS (Redundant Storage)","Raspberry Pi","VPS (Hybrid)",["Date","2023-01-01T00:00:00.000Z"],[76,77,78,79,80,81],"Hybrid architecture across on-prem and VPS","Segmented production networking with VLANs and edge IPS","Proxmox + Portainer stacks for container orchestration","Redundant storage for data integrity","Public access via Cloudflare Tunnels and domain separation","Stable multi-user operations for 2+ years",4,"### Overview\n\nMy home lab is not just a place for tinkering and experimentation, it runs real production workloads with stakeholders depending on its services. These systems are engineered with stability, extensibility, and security as guiding principles. For more than two years they have reliably supported around 15 users with zero unplanned downtime.\n\nThe platform is built on a hybrid architecture:\n\n- **On‑prem**: a Docker container host managed by Portainer with redundant storage, alongside a Raspberry Pi Kubernetes cluster with its own redundancy.\n- **Cloud VPS**: a small set of public‑facing nodes for uptime‑critical and latency‑sensitive workloads.\n\nThis split balances control, latency, and availability. Storage‑heavy services such as my self‑hosted photo server stay local for performance and data privacy. High‑availability services run on VPS nodes to provide resilience and external ingress. Today that is only a handful of VPS servers, but the design can scale smoothly into a full distributed swarm in the cloud if demand grows.\n\nIn this article I will focus on the on‑prem solution. You can read more about the VPS side of the platform [here](/projects/production-homelab-part-2-vps).\n\n### Proxmox and Portainer\n\nI run Proxmox VE as the virtualization layer and use Portainer on top to manage containers as versioned stacks. I like Portainer because it centralizes environment variables, health checks, and life-cycle operations in a way that keeps recovery fast and predictable. Most production services live here today. I am planning to continue to migrate more services to Kubernetes that are not storage or extremely resource intensive.\n\n- Workloads: media and data services, internal apps, utility services.\n- Orchestration: Portainer stacks with compose files.\n- Operations: rolling restarts, labels for routing, and per‑stack resource limits.\n\n### Raspberry Pi Kubernetes cluster\n\nI also run a 5‑node Raspberry Pi cluster dedicated to Kubernetes workloads. The cluster uses Traefik for ingress and Longhorn for distributed block storage across the Pis.\n\n- Ingress: Traefik handles internal and external routing. Cloudflared terminates tunnels externally and forwards to Traefik; internal DNS patterns also point to Traefik for consistent service discovery inside the LAN.\n- Storage: Longhorn provides replicated volumes across the Raspberry Pis.\n- GitOps today: Flux manages Kubernetes manifests declaratively from Git.\n- Migration: Actively migrating to Pulumi's Kubernetes Operator to shift GitOps to IaC-driven workflows while retaining declarative state in the repo.\n- Control plane: Single master node to conserve resources; etcd is regularly backed up. This meets the MTTR requirements for current workloads. If the cluster becomes more critical, I will scale up resources and move to a three‑node control plane for quorum.\n- Caution: I would not run a single‑master control plane in a true production environment; this is a deliberate tradeoff for homelab resource efficiency.\n\n### Networking and security\n\nI designed the network to reduce blast radius and keep sensitive segments isolated. I have also designed the security to be multilayer reducing many of the common attack vectors.\n\n- Segmentation: production services run in dedicated VLANs; east‑west access is restricted.\n- Edge protection: an IPS at the perimeter inspects traffic before it reaches internal tiers.\n- Public access: Cloudflare Tunnels publish selected services, mapped to separate top‑level domains for clean separation by audience and risk profile.\n- Critical system patching alerts, and monthly maintenance windows.\n\n### Storage and reliability\n\nI combine multiple layers to protect data and speed recovery:\n\n- Proxmox storage: mirrored ZFS pool (ashift tuned for SSDs) with regular scrub cadence; snapshots on critical VMs and encrypted off‑box replication for disaster recovery.\n- Kubernetes storage: Longhorn with 5 replica volumes across Pis; recurrent snapshots and scheduled backups to external storage; automatic rebuild on node loss.\n- Databases: periodic logical backups in addition to volume‑level protection; tested restores.\n- Integrity and capacity: SMART monitoring, ZFS health alerts, and capacity thresholds with notifications before saturation.\n- Backup policy: 3‑2‑1 inspired—at least three copies across two media types with one off‑site (object storage or VPS), with retention tuned per service RPO/RTO. This policy is of course for the most sensitive data only. Non critical data is spread across two media.\n\nThese controls reduce the risk of silent corruption, node failure, or operator error and keep MTTR low for the current criticality level.\n\n### Operations and stability\n\nThis environment has been stable for over two years. I monitor service health through Portainer, keep images current, and stage changes as stack updates. The design keeps day‑to‑day work simple while supporting iterative upgrades.\n\nAll production services are continuously monitored with alerting on availability, latency, and resource thresholds. Critical workloads include self‑healing mechanisms such as health checks with automated restarts/redeploys and rollback on failed health to minimize MTTR.\n\n### Why this project stands out to me\n\nIt blends pragmatic homelab constraints with production‑grade practices: layered storage and backups, real ingress and\nDNS topology, and an evolution from GitOps to IaC with Pulumi—all validated by multi‑year uptime serving real users.\nIt’s a living platform for testing tradeoffs, scaling patterns, and reliability strategies on hardware/platforms most engineers have access to.","src/data/projects/en/home-lab-production-workflow/index.mdx",[86],"../home-lab-production-workflow/image.png","fba7a4d5acf47d46","en/custom-report-generator",{"id":88,"data":90,"body":110,"filePath":111,"assetImports":112,"digest":114,"deferredRender":23},{"title":91,"description":92,"image":93,"technologies":94,"completionDate":101,"keyFeatures":102,"order":109,"draft":52},"Custom CI/CD Report Generator","Python-based report generator that unified disparate YAML outputs from open-source quality tools, validated and normalized the data, and produced fast, static HTML dashboards with pagination, search, and severity filtering-deployed via GitLab CI and served on GitLab Pages.","__ASTRO_IMAGE_../custom-report-generator/image.png",[95,38,96,97,98,99,100],"Python","YAML","HTML","CSS","Vanilla JS","Static Site Generation",["Date","2025-06-17T00:00:00.000Z"],[103,104,105,106,107,108],"Unified multiple tool-specific YAML outputs into a single normalized schema","Python validators and converters to ensure consistent structure and types","Static HTML generation with modern CSS styling; no JS frameworks","Client-side storage (localStorage) for report data and user settings","Search, pagination, and severity-based filtering for large datasets","Deployed as a GitLab CI step and hosted on GitLab Pages",3,"### Problem\n\nEngineering and management needed actionable visibility into code quality findings (bugs, security issues, and hygiene)\nwithout buying a commercial platform. We had several open-source tools producing useful reports, but each emitted\ndifferent YAML structures; making aggregation and comparison hard. Raw YAML data also does not provide a very great\nexperience for understanding the information.\n\n### Approach\n\nI built a Python report generator that ran as a dedicated job in GitLab CI. I kept the pipeline simple and transparent so developers could reason about and maintain it:\n\n- Normalize: I parsed each tool’s YAML and mapped fields into a shared schema (category, severity, file, line, rule, message, tags, timestamps).\n- Validate: I enforced schema and types and gracefully flagged and skipped malformed records while surfacing counts in the summary.\n- Convert: I wrote normalized artifacts to a compact JSON bundle for in-browser consumption and to pre-rendered HTML for immediate viewing.\n- Generate: I produced static HTML pages with lightweight, modern CSS and a simple component structure.\n\n### UX and performance\n\n- I used simple links and CSS-driven active states to simulate SPA-like navigation, which kept the site fast and accessible without a framework.\n- I stored normalized data and user preferences (selected filters, page size) in the browser to speed reloads.\n- I added pagination and a debounced search to keep large reports responsive.\n- I avoided UI libraries on purpose - just a few kilobytes of vanilla JS for the interactivity I needed.\n\n### CI/CD and hosting\n\n- I wired the pipeline step to validate inputs, generate artifacts, and publish the static site to GitLab Pages.\n- I produced artifacts like normalized JSON, per-tool diagnostics, and the HTML/CSS/JS bundle.\n- I exposed summary stats to later stages and merges (e.g., total issues, severity distribution) to enable simple quality gates.\n\n### What I delivered\n\n- I gave the team a consistent, comparable view of issues across multiple tools without a paid product.\n- I built a fast, portable reporting site that works on any static host.\n- I added clear guardrails for data quality, keeping the pipeline resilient even when upstream report formats change.\n\n### Why this project stands out to me\n\nI like how this balances impact and simplicity: organization-wide visibility delivered through a small, robust Python pipeline that normalized messy inputs, generated fast static dashboards, and shipped through CI without vendor lock-in or runtime overhead. I deliberately avoided frameworks - just efficient HTML/CSS/JS and client-side storage - so large reports feel instant while keeping the system easy to maintain.","src/data/projects/en/custom-report-generator/index.mdx",[113],"../custom-report-generator/image.png","2e91a6306dab9a7c","en/portfolio-website",{"id":115,"data":117,"body":141,"filePath":142,"assetImports":143,"digest":145,"deferredRender":23},{"title":118,"description":119,"image":120,"technologies":121,"completionDate":131,"keyFeatures":132,"order":140,"draft":52},"Portfolio Website (This site)","A production-grade Astro + TypeScript portfolio with content collections, i18n, SEO, performance optimizations, and a GitOps deployment to a hardened VPS behind Cloudflare Tunnels and Traefik.","__ASTRO_IMAGE_../portfolio-website/image.png",[122,123,124,125,126,127,68,128,67,129,130],"Astro","TypeScript","MDX","Tailwind CSS","ESLint 9 + Prettier","Astro Content Collections","Traefik","Coolify (GitOps)","Cloudflare R2 (backups)",["Date","2025-08-01T00:00:00.000Z"],[133,134,135,136,137,138,139],"Astro + TypeScript architecture with MDX content collections","Accessible, responsive components and strong SEO defaults","i18n with data and text translation separation","Astro assets pipeline for optimized images","GitOps deployment with PR previews and merge-driven releases","Hardened VPS hosting behind Cloudflare Tunnels and Traefik","Monitoring and backup posture consistent with my platform",6,"### Overview\n\nThis portfolio is built to the same production standards I use for client work: fast, accessible, and maintainable, with a clean content pipeline and automated deployment. It uses Astro with TypeScript and MDX content collections to keep content structured and portable, Tailwind CSS to keep styles consistent and lean, and a light i18n framework to support localized data and text translations.\n\nThe project integrates SEO fundamentals (structured data, canonical and hreflang tags, RSS), performance-conscious images with `astro:assets`, and a component library emphasizing semantic HTML.\n\n### Architecture highlights\n\n- Framework: Astro for hybrid-SSR/static output, islands where needed, and zero-JS-by-default pages.\n- Language and tooling: TypeScript with strictness, ESLint 9, and Prettier to keep the codebase consistent and safe.\n- Content: Astro Content Collections for project/blog data with schemas; MDX for rich content; predictable routes and slugs.\n- Styling: Tailwind CSS with a minimal global layer and utility-first components; careful focus states and keyboard navigation.\n- i18n: Data translations via `src/config/translationData.json.ts` and route translations; locale-aware utilities and\n  link generation. This will support future growth if this site needs to reach non-english speaking audiences.\n- Performance: Static-first pages, small islands, optimized images, and tidy CSS. Avoids client runtime JS unless necessary.\n\n### Deployment and operations\n\nThis site ships through a GitOps pipeline:\n\n- PR previews: Each pull request generates an ephemeral preview site for content and UI review.\n- Merge-to-main deploy: Merging triggers an automated build and deploy via Coolify on my VPS.\n- Ingress: The app runs behind Traefik (per-environment/domain separation). Traefik handles routing, middlewares, and TLS at the edge via Cloudflare.\n- Exposure: Cloudflare Tunnels terminate at the edge; no ports are opened to the internet and the origin IP remains private.\n- Reliability: Monitoring notifies on deploys and health; backups align with my broader platform posture (object storage via Cloudflare R2 for artifacts/state where applicable).\n\n### Developer experience\n\n- Automated formatting, linting, and type safety make content edits low-risk and fast.\n- Content authorship uses predictable MDX + schema-driven frontmatter, so adding projects or blog posts remains frictionless.\n- Aliased imports and clear folder structure keep components discoverable and decoupled.\n\n### Why this project stands out to me\n\nIt’s a practical demonstration of end-to-end product thinking: strong foundations (a11y, SEO, perf), clean content workflows, and a secure GitOps deployment model on a hardened VPS—no exposed ports, Cloudflare edge protections, and Traefik policy control. It also shows restraint: minimal JS on the client, right-sized automation, and infrastructure that can scale without adding avoidable complexity.","src/data/projects/en/portfolio-website/index.mdx",[144],"../portfolio-website/image.png","2e8e680396d10f51","en/rke-2-cluster-factory",{"id":146,"data":148,"body":166,"filePath":167,"assetImports":168,"digest":170,"deferredRender":23},{"title":149,"description":150,"image":151,"technologies":152,"completionDate":157,"keyFeatures":158,"order":165,"draft":52},"RKE2 Cluster Factory","Automated Multi-Cluster Kubernetes Platform with Rancher, ArgoCD, Ansible, and Terraform","__ASTRO_IMAGE_../rke-2-cluster-factory/image.png",[153,154,35,155,156,34],"Rancher","RKE2","OpenTofu","MetalLB",["Date","2025-05-31T00:00:00.000Z"],[159,160,161,162,163,164],"DevOps","Infrastructure as Code","Fully Automated Cluster Lifecycle","Dynamic Configuration with Ansible","GitOps-Enabled with ArgoCD","Production-Ready",2,"#### Overview\n\nThis project is a fully automated system for provisioning, managing, and scaling Kubernetes clusters using RKE2, Rancher, Ansible, and Terraform. It’s designed to turn bare metal or virtual machines into a production-ready, GitOps-enabled Kubernetes environment with minimal manual intervention. Primarily focused taking the complexity out of standing up multi cluster Kubernetes environments. I can not attach the actual code here because it is not publicly accessible.&#x20;\n\n---\n\n#### Technical Highlights\n\n- **End-to-End Automation:**\\\n  The entire lifecycle—from initial cluster bootstrap to multi-cluster management and app deployment—is automated. No click-ops, no snowflake clusters.\n- **Ansible-Driven Bootstrap:**\\\n  Ansible playbooks handle the initial provisioning of a parent RKE2 cluster. This includes installing Rancher for cluster management, MetalLB for load balancing, NGINX Ingress for traffic routing, and other supporting tools. The playbooks are modular and idempotent, making them easy to extend or rerun safely.\n- **Terraform + Rancher API:**\\\n  Terraform takes over to interact with Rancher’s API, dynamically creating and registering additional RKE2 clusters as managed resources. This approach allows for infrastructure-as-code management of not just the clusters, but also their Rancher integration.\n- **Dynamic Cluster Configuration:**\\\n  For each new child cluster, Terraform generates Ansible inventories and playbooks on the fly. Ansible then configures each cluster, installing MetalLB, ArgoCD (for GitOps), and any other required tools. This ensures every cluster is consistently configured and production-ready.\n- **GitOps Ready:**\\\n  ArgoCD is deployed automatically to all clusters, enabling continuous delivery and declarative application management from the start. I chose to use a hub and spoke model for this implementation. The core ArgoCD application lived in our root cluster, and the spokes were automatically deployed to each of the child clusters.&#x20;\n- **Scalable and Repeatable:**\\\n  The system is designed to scale horizontally—spin up as many clusters as you need, all managed centrally through Rancher. The automation is fully repeatable, making it easy to rebuild or expand your environment.\n- **Bare Metal Friendly:**\\\n  MetalLB provides load balancing without the need for cloud provider integrations, making this stack ideal for on-premises or hybrid environments.\n\n---\n\n#### Why This Project Stands Out to me\n\n- **Demonstrates advanced automation and orchestration skills across multiple tools and platforms.**\n- **Bridges the gap between infrastructure provisioning and application delivery.**\n- **Showcases expertise in Kubernetes, infrastructure as code, and DevOps best practices.**\n- **Ready for real-world, production-grade use cases—on-prem, cloud, or hybrid.**","src/data/projects/en/rke-2-cluster-factory/index.mdx",[169],"../rke-2-cluster-factory/image.png","5325f0e07456f3b0","en/production-homelab-part-2-vps",{"id":171,"data":173,"body":195,"filePath":196,"assetImports":197,"digest":199,"deferredRender":23},{"title":174,"description":175,"image":176,"technologies":177,"completionDate":183,"keyFeatures":184,"order":194,"draft":52},"Production Homelab (Part 2 - VPS)","Uptime-focused VPS layer: hardened Ubuntu 24.04, Traefik ingress behind Cloudflared (no open ports, hidden IP), Coolify with a GitOps workflow, per‑client projects and shared services, preview environments per PR, multi‑cloud diversity, and layered backups to Cloudflare R2 for fast recovery.","__ASTRO_IMAGE_../production-homelab-part-2-vps/image.png",[178,128,179,180,39,181,67,182],"Ubuntu 24.04 (hardened)","Cloudflared (Cloudflare Tunnels)","Coolify","Cloudflare R2","VPS (multi‑cloud)",["Date","2024-08-01T00:00:00.000Z"],[185,186,187,188,189,190,191,192,193],"Hardened Ubuntu 24.04 with minimal surface area","Ingress via Cloudflared tunnels (no open ports, hidden origin IP)","Traefik routing with domain separation per client","Coolify app platform with PR preview environments","GitOps deployment from repo to production on merge","Shared services project (web checks, time tracking, etc.)","Multi‑cloud VPS diversity for provider resilience","Layered, redundant backups to Cloudflare R2 for low MTTR","Many client sites actively serve ~10,000 visits/week",5,"### Overview\n\nMy home lab is not just a place for tinkering and experimentation, it runs real production workloads with stakeholders depending on its services. These systems are engineered with stability, extensibility, and security as guiding principles. For more than two years they have reliably supported around 15 users with zero unplanned downtime.\n\nThe platform is built on a hybrid architecture:\n\n- **On‑prem**: a Docker container host managed by Portainer with redundant storage, alongside a Raspberry Pi Kubernetes cluster with its own redundancy.\n- **Cloud VPS**: a small set of public‑facing nodes for uptime‑critical and latency‑sensitive workloads.\n\nThis split balances control, latency, and availability. Storage‑heavy services such as my self‑hosted photo server stay local for performance and data privacy. High‑availability services run on VPS nodes to provide resilience and external ingress. Today that is only a handful of VPS servers, but the design can scale smoothly into a full distributed swarm in the cloud if demand grows.\n\nIn this article I focus on the VPS side of the platform. To see the on‑prem solution, click [here](/projects/home-lab-production-workflow).\n\n### VPS platform design\n\nI currently operate two VPS servers dedicated to workloads needing higher uptime. SLAs comfortably allow up to one day of downtime, so there is no active failover today; however, the design can expand into a Swarm cluster when SLAs tighten.\n\n- OS baseline: Hardened Ubuntu 24.04 with reduced package footprint and opinionated security defaults.\n- Ingress and exposure: Cloudflared fronts Traefik, so no ports are opened to the internet and the VPS origin IPs remain private. Domains are separated per client to isolate tenants and simplify routing.\n- App platform: Coolify manages services, builds, and deployments with a GitOps approach. Each pull request spins up an ephemeral preview site; merging to main promotes to production automatically.\n- Projects model: A shared‑services project (web checking, time tracking, and supporting utilities) alongside separate per‑client projects for clear boundaries and scaling.\n- Multi‑cloud: VPS instances are split across two cloud providers for diversity and provider‑level resilience.\n\n### Operations and stability\n\n- Monitoring: Active monitoring is in place; alerts are sent to my Discord on most operational events (deployments, failures, resource thresholds, and health checks).\n- Backups and recovery: Coolify backups and relevant data sources write to Cloudflare R2. Additional layers of backup reduce catastrophic‑loss risk and enable fast restore paths.\n- Release flow: PR → ephemeral preview → code review → merge to main → automated deploy to production.\n- Uptime posture: Over the last year there has been zero unplanned downtime for client sites. Many client sites actively serve ~10,000 visits per week. Current SLAs accept up to 24 hours MTTR; architecture is optimized for simplicity and rapid manual recovery over idle redundancy. Swarm‑based HA is the planned next step if SLAs tighten.\n\n### Security and networking\n\n- No inbound ports: All exposure is via Cloudflare Tunnels; WAF and zero‑trust policies can be applied at the edge.\n- Traefik policies: Explicit routing, middlewares, and domain separation per client and per environment (preview vs. prod).\n- Hardening: Baseline OS hardening, least‑privilege credentials, secrets management within platform, and periodic patching windows.\n\n### Roadmap\n\n- Scale to Swarm for active failover/high availability when business requirements increase.\n- Expand shared services (synthetic checks, observability exporters) and standardize client project templates.\n- Add additional object‑storage regions and scheduled DR tests to further reduce recovery risk and time.\n\n### Why this project stands out to me\n\nIt’s a pragmatic, end‑to‑end hosting platform for local businesses: multi‑cloud VPS, zero‑exposed ports via Cloudflared, disciplined routing with Traefik, GitOps‑driven deployments in Coolify, and preview environments that make reviews fast and safe. It balances real‑world uptime needs with simplicity and cost, while leaving a clear path to clustered high availability.\n\nI also appreciate that it deliberately avoids unnecessary complexity in build and deploy. There was no need to over‑provision systems for the current requirements. I could have gone wild with a fully clustered, HA‑everywhere setup from day one and stalled the project; instead, trade‑offs were explicit, documented, and right‑sized for today. Don’t overcomplicate when there’s no need to overcomplicate.","src/data/projects/en/production-homelab-part-2-vps/index.mdx",[198],"../production-homelab-part-2-vps/image.png","da7df65917ab1b7c","en/mr-builders",{"id":200,"data":202,"body":218,"filePath":219,"assetImports":220,"digest":222,"deferredRender":23},{"title":203,"description":204,"image":205,"technologies":206,"demoUrl":209,"githubUrl":210,"completionDate":211,"keyFeatures":212,"order":217,"draft":52},"MR Builders Website","Static landing page for local construction company.","__ASTRO_IMAGE_../mr-builders/image.png",[122,207,208,67],"React","TailwindCSS","https://mr-builders.com","https://github.com/Glucktek/MRBuilders-web",["Date","2025-03-10T00:00:00.000Z"],[213,214,215,216],"SEO optimization suggestions","Content performance analytics","Clean and professional design tailored to the construction industry.","Fully responsive layout for mobile, tablet, and desktop users.",7,"### **Project Overview**\n\nThe MR Builders website is a professional online platform developed for a construction business to showcase their services, portfolio, and contact details. Designed with a user-friendly interface, it aims to enhance customer engagement and provide a seamless browsing experience for potential clients.\n\n### **Technical Implementation**\n\nThe website is built using Astro, ensuring fast and scalable static site generation. JavaScript is utilized for interactive features, while Docker is employed to streamline deployment and containerization for a reliable and efficient development workflow.\\\n\\\n**Role in Project:**\nAs the developer, I managed the end-to-end implementation, from front-end design to deployment, ensuring a seamless and high-performing website.","src/data/projects/en/mr-builders/index.mdx",[221],"../mr-builders/image.png","12a102cf692de1e6","resume",["Map",225,226],"en/resume",{"id":225,"data":227,"filePath":534,"assetImports":535,"digest":581},{"diplomas":228,"certifications":233,"experience":245,"hardSkills":316,"softSkills":345,"languages":369,"tools":373},[229],{"title":230,"school":231,"year":232},"Associate of Science - Computer Programming Technologies","Vincennes University",2016,[234,237,240,242],{"title":235,"year":236},"AWS Solutions Architect",2021,{"title":238,"year":239},"MTA: Software Development Fundamentals",2018,{"title":241,"year":239},"MTA: Networking Fundamentals",{"title":243,"year":244},"CompTIA Security+",2017,[246,263,274,289,304],{"title":247,"company":248,"companyImage":249,"dates":250,"location":251,"responsibilities":252},"Sr Software DevOps Engineer","Magnit","__ASTRO_IMAGE_@images/resume/magnit-logo.png","September 2022 - August 2025","California, United States",[253,254,255,256,257,258,259,260,261,262],"Mentored junior DevOps engineers through pairing, design reviews, and structured learning paths; accelerated their readiness to own services independently and reduced handoff overhead across teams.","Led cross-functional initiatives with AppSec, SRE, and platform teams to align guardrails with product velocity; established metrics for change fail rate, MTTR, and deployment frequency and used them to drive continuous improvement.","Facilitated architectural decisions, and technical design sessions; ensured decisions were documented, reversible, and tied to measurable business outcomes.","Championed a coaching culture by instituting regular knowledge share sessions with engineers. I aslo lead incident postmortem walkthroughs to improve shared understanding of reliability practices and reduced repeat incidents.","Orchestrated end-to-end cloud automation for an entire department using Pulumi (TypeScript), delivering fully reproducible, policy-compliant infrastructure with zero-touch environments.","Engineered robust CI/CD pipelines for a microservices ecosystem, standardizing builds, testing, security scanning, and deployments to dramatically increase release reliability and speed.","Stood up a Rancher-managed Kubernetes platform (RKE2/k3s), automating management-cluster bootstrap and downstream cluster provisioning with Ansible and Terraform for repeatable, auditable cluster lifecycle management.","Implemented a production-grade GitOps operating model with ArgoCD, enforcing declarative deployments, drift detection, and automated rollbacks across environments.","Automated 30% of developer workflows, eliminating manual toil, accelerating feedback loops, and enabling self-service delivery for product teams.","Established guardrails and golden paths (templates, pipelines, policies) that scaled across teams, improving security posture and consistency without slowing delivery.",{"title":264,"company":265,"companyImage":266,"dates":267,"location":268,"responsibilities":269},"Senior DevOps Engineer","Voxr AI Corp","__ASTRO_IMAGE_@images/resume/voxr-logo.png","February 2024 - June 2025","Louisville Metropolitan Area",[270,271,272,273],"Weekend and evening contract work","Built out entire CI/CD process","Designed scalable robust cloud platform to allow for cost effectiveness in early stages of application development","Consulted on DevOps best practices",{"title":275,"company":276,"companyImage":277,"dates":278,"location":279,"responsibilities":280},"Principal DevOps Engineer ","Quanterix","__ASTRO_IMAGE_@images/resume/quanterix-logo.png","September 2021 - August 2022","Boston, Massachusetts, United States",[281,282,283,284,285,286,287,288],"Built the DevOps function from the ground up: helped define team charter, hiring profile, operating model, and technical standards; onboarded and coached engineers to a high-performance cadence.","Stood up and led the DevOps team, establishing ownership boundaries (platform, pipelines, SRE-lite) and SLAs that clarified accountability and improved stakeholder confidence.","Managed a strategic third-party agency relationship: set outcomes and KPIs, negotiated scope and SOWs, implemented sprint cadences, and enforced quality gatesdelivering on time and within budget.","Pioneered DevOps across the organizationestablished the philosophy, operating model, and end-to-end pipelines; evangelized best practices to leadership and engineering to drive company-wide adoption.","Converted manual, days-long release processes into fully automated pipelines, shrinking deployments and validations to minutes while increasing reliability and repeatability.","Designed developer-centric workflows (branching, reviews, environments, artifact/version strategy) that streamlined delivery, improved feedback loops, and unlocked self-service.","Implemented Infrastructure as Code with Pulumi, enabling consistent, policy-compliant provisioning and rapid environment spin-up across teams.","Built and evolved hybrid cloud foundations in AWS and Azure, including networking, identity, security guardrails, and platform services to support product growth.",{"title":290,"company":291,"companyImage":292,"dates":293,"location":294,"responsibilities":295},"Lead DevOps Engineer","EAGLE6","__ASTRO_IMAGE_@images/resume/rocket-icon.png","March 2018 - September 2021","Louisville, Kentucky Area",[296,297,298,299,300,301,302,303],"Developed custom tools to meet organizational needs in Golang and Python","Developed build scripts to allow developers to build the product locally using Ansible","Developed automated configuration of entire AWS infrastructure using AWX, RedHat's open-source variant of Ansible tower","Converted the provisioning of all legacy infrastructure to Terraform and AWX","Continued development of CI/CD pipeline with Jenkins to ensure security and quality are maintained with all software releases","Managed Octopus Server to orchestrate releases to all environments from development to Demo servers","Managed a fleet of over 200 Linux servers","Performed monthly red team security attacks against products to ensure no externally facing vulnerabilities",{"title":305,"company":306,"companyImage":292,"dates":307,"location":294,"responsibilities":308},"Full-stack Developer","ALTOUR","September 2017 - January 2018",[309,310,311,312,313,314,315],"Acted as DevOps engineer for the team","Managed on-premises Linux environments from development to production","Automated deployments to production systems","Internal corporate development","HTML5/JavaScript Front End Design","Built MVC web applications on the CodeIgniter framework","Linux system administration (CentOS, RedHat enterprise derivative)",[317,320,322,324,326,329,331,333,335,337,340,342],{"skill":318,"percentage":319},"AWS/Cloud Platforms",96,{"skill":33,"percentage":321},95,{"skill":323,"percentage":321},"CI/CD Pipelines",{"skill":325,"percentage":321},"Linux Administration",{"skill":327,"percentage":328},"Pulumi/Terraform",90,{"skill":330,"percentage":328},"Docker/Containers",{"skill":95,"percentage":332},85,{"skill":334,"percentage":332},"Golang",{"skill":336,"percentage":332},"GitOps/ArgoCD",{"skill":338,"percentage":339},"TypeScript/JavaScript",75,{"skill":122,"percentage":341},65,{"skill":343,"percentage":344},"React/React Native",50,[346,349,352,355,357,360,363,366],{"skill":347,"icon":348},"Leadership","tabler/star-filled",{"skill":350,"icon":351},"Team Collaboration","tabler/user",{"skill":353,"icon":354},"Mentoring","tabler/clipboard",{"skill":356,"icon":354},"Vendor Management",{"skill":358,"icon":359},"Stakeholder Management","tabler/briefcase",{"skill":361,"icon":362},"Problem Solving","tabler/bulb",{"skill":364,"icon":365},"Process Improvement","tabler/arrow-up-right",{"skill":367,"icon":368},"Strategic Planning","tabler/layout",[370],{"language":371,"level":372},"English",10,[374,379,383,386,389,393,397,400,404,407,410,413,417,421,425,429,432,435,438,441,445,449,453,456,460,464,467,471,474,478,482,486,490,494,498,501,505,509,513,517,521,525,528,531],{"name":375,"category":376,"image":377,"link":378},"AWS","Cloud","__ASTRO_IMAGE_@images/resume/aws-logo.png","https://aws.amazon.com",{"name":95,"category":380,"image":381,"link":382},"Programming","__ASTRO_IMAGE_@images/resume/python-logo.png","https://python.org",{"name":334,"category":380,"image":384,"link":385},"__ASTRO_IMAGE_@images/resume/golang-logo.png","https://golang.org",{"name":123,"category":380,"image":387,"link":388},"__ASTRO_IMAGE_@images/resume/typescript-logo.png","https://typescriptlang.org",{"name":390,"category":380,"image":391,"link":392},"JavaScript","__ASTRO_IMAGE_@images/resume/javascript-logo.png","https://developer.mozilla.org/en-US/docs/Web/JavaScript",{"name":394,"category":380,"image":395,"link":396},"Bash","__ASTRO_IMAGE_@images/resume/bash-logo.png","https://www.gnu.org/software/bash/",{"name":122,"category":380,"image":398,"link":399},"__ASTRO_IMAGE_@images/resume/astro-logo.svg","https://astro.build",{"name":401,"category":380,"image":402,"link":403},"Bun","__ASTRO_IMAGE_@images/resume/bun-logo.svg","https://bun.sh",{"name":207,"category":380,"image":405,"link":406},"__ASTRO_IMAGE_@images/resume/react-logo.png","https://react.dev",{"name":408,"category":380,"image":405,"link":409},"React Native","https://reactnative.dev",{"name":208,"category":380,"image":411,"link":412},"__ASTRO_IMAGE_@images/resume/tailwind-logo.png","https://tailwindcss.com",{"name":414,"category":380,"image":415,"link":416},"HTML5","__ASTRO_IMAGE_@images/resume/html5-logo.png","https://developer.mozilla.org/en-US/docs/Web/HTML",{"name":418,"category":380,"image":419,"link":420},"CSS3","__ASTRO_IMAGE_@images/resume/css3-logo.png","https://developer.mozilla.org/en-US/docs/Web/CSS",{"name":422,"category":376,"image":423,"link":424},"ECS","__ASTRO_IMAGE_@images/resume/ecs-logo.png","https://aws.amazon.com/ecs/",{"name":426,"category":376,"image":427,"link":428},"EKS","__ASTRO_IMAGE_@images/resume/eks-logo.png","https://aws.amazon.com/eks/",{"name":430,"category":376,"image":377,"link":431},"CloudFormation","https://aws.amazon.com/cloudformation/",{"name":67,"category":159,"image":433,"link":434},"__ASTRO_IMAGE_@images/resume/docker-logo.png","https://docker.com",{"name":33,"category":159,"image":436,"link":437},"__ASTRO_IMAGE_@images/resume/kubernetes-logo.png","https://kubernetes.io",{"name":36,"category":159,"image":439,"link":440},"__ASTRO_IMAGE_@images/resume/helm-logo.png","https://helm.sh",{"name":442,"category":159,"image":443,"link":444},"Jenkins","__ASTRO_IMAGE_@images/resume/jenkins-logo.png","https://jenkins.io",{"name":446,"category":159,"image":447,"link":448},"GitHub Actions","__ASTRO_IMAGE_@images/resume/github-logo.png","https://github.com/features/actions",{"name":450,"category":159,"image":451,"link":452},"GitLab CI","__ASTRO_IMAGE_@images/resume/gitlab-logo.png","https://docs.gitlab.com/ee/ci/",{"name":34,"category":159,"image":454,"link":455},"__ASTRO_IMAGE_@images/resume/argocd-logo.png","https://argoproj.github.io/cd",{"name":457,"category":159,"image":458,"link":459},"FluxCD","__ASTRO_IMAGE_@images/resume/fluxcd-logo.png","https://fluxcd.io",{"name":461,"category":159,"image":462,"link":463},"Linux","__ASTRO_IMAGE_@images/resume/linux-logo.png","https://kernel.org",{"name":37,"category":159,"image":465,"link":466},"__ASTRO_IMAGE_@images/resume/terraform-logo.png","https://terraform.io",{"name":468,"category":159,"image":469,"link":470},"Pulumi","__ASTRO_IMAGE_@images/resume/pulumi-logo.svg","https://pulumi.com",{"name":35,"category":159,"image":472,"link":473},"__ASTRO_IMAGE_@images/resume/ansible-logo.png","https://ansible.com",{"name":475,"category":159,"image":476,"link":477},"Grafana","__ASTRO_IMAGE_@images/resume/grafana-logo.png","https://grafana.com",{"name":479,"category":159,"image":480,"link":481},"Prometheus","__ASTRO_IMAGE_@images/resume/prometheus-logo.png","https://prometheus.io",{"name":483,"category":159,"image":484,"link":485},"Vault","__ASTRO_IMAGE_@images/resume/vault-logo.svg","https://vaultproject.io",{"name":487,"category":159,"image":488,"link":489},"Packer","__ASTRO_IMAGE_@images/resume/packer-logo.svg","https://packer.io",{"name":491,"category":159,"image":492,"link":493},"Loki","__ASTRO_IMAGE_@images/resume/logo-loki.svg","https://grafana.com/oss/loki",{"name":495,"category":159,"image":496,"link":497},"Redis","__ASTRO_IMAGE_@images/resume/redis-logo.png","https://redis.io",{"name":153,"category":159,"image":499,"link":500},"__ASTRO_IMAGE_@images/resume/rancher-logo.svg","https://rancher.com",{"name":502,"category":159,"image":503,"link":504},"Podman","__ASTRO_IMAGE_@images/resume/podman-logo.svg","https://podman.io",{"name":506,"category":159,"image":507,"link":508},"New Relic","__ASTRO_IMAGE_@images/resume/newrelic-logo.svg","https://newrelic.com",{"name":510,"category":159,"image":511,"link":512},"PostgreSQL","__ASTRO_IMAGE_@images/resume/postgresql-logo.png","https://postgresql.org",{"name":514,"category":159,"image":515,"link":516},"MySQL","__ASTRO_IMAGE_@images/resume/mysql-logo.png","https://mysql.com",{"name":518,"category":159,"image":519,"link":520},"SQLite","__ASTRO_IMAGE_@images/resume/sqlite-logo.png","https://sqlite.org",{"name":522,"category":159,"image":523,"link":524},"K3s","__ASTRO_IMAGE_@images/resume/k3s-logo.svg","https://k3s.io",{"name":154,"category":159,"image":526,"link":527},"__ASTRO_IMAGE_@images/resume/rke2-logo.svg","https://docs.rke2.io",{"name":529,"category":159,"image":480,"link":530},"MinIO","https://min.io",{"name":128,"category":159,"image":532,"link":533},"__ASTRO_IMAGE_@images/resume/traefik-logo.png","https://traefik.io","src/data/resume/en/resume/index.json",[536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580],"@images/resume/magnit-logo.png","@images/resume/voxr-logo.png","@images/resume/quanterix-logo.png","@images/resume/rocket-icon.png","@images/resume/aws-logo.png","@images/resume/python-logo.png","@images/resume/golang-logo.png","@images/resume/typescript-logo.png","@images/resume/javascript-logo.png","@images/resume/bash-logo.png","@images/resume/astro-logo.svg","@images/resume/bun-logo.svg","@images/resume/react-logo.png","@images/resume/tailwind-logo.png","@images/resume/html5-logo.png","@images/resume/css3-logo.png","@images/resume/ecs-logo.png","@images/resume/eks-logo.png","@images/resume/docker-logo.png","@images/resume/kubernetes-logo.png","@images/resume/helm-logo.png","@images/resume/jenkins-logo.png","@images/resume/github-logo.png","@images/resume/gitlab-logo.png","@images/resume/argocd-logo.png","@images/resume/fluxcd-logo.png","@images/resume/linux-logo.png","@images/resume/terraform-logo.png","@images/resume/pulumi-logo.svg","@images/resume/ansible-logo.png","@images/resume/grafana-logo.png","@images/resume/prometheus-logo.png","@images/resume/vault-logo.svg","@images/resume/packer-logo.svg","@images/resume/logo-loki.svg","@images/resume/redis-logo.png","@images/resume/rancher-logo.svg","@images/resume/podman-logo.svg","@images/resume/newrelic-logo.svg","@images/resume/postgresql-logo.png","@images/resume/mysql-logo.png","@images/resume/sqlite-logo.png","@images/resume/k3s-logo.svg","@images/resume/rke2-logo.svg","@images/resume/traefik-logo.png","378c40ff3582b4ae","otherPages",["Map",584,585],"en/privacy-policy",{"id":584,"data":586,"body":589,"filePath":590,"digest":591,"deferredRender":23},{"title":587,"description":588,"draft":52},"Privacy Policy","Example privacy policy for this template","This privacy policy sets out how Example LLC collects, processes, and uses your Personal Information through your use of our Services.\n\n**This is purely for example. Please consult a lawyer for your own privacy policy.**\n\n## Personal Information Collection\n\nWe only collect and use Personal Information to the extent necessary to provide you with the Services. We collect Personal Information for the Website to provide you with a better online experience.\n\n## Personal Information Retention\n\nWe keep Personal Information for the shortest time necessary to provide you with Services and to meet all our legal and compliance obligations. To determine this retention period, we take into account (i) the nature of the Personal Information gathered; and (ii) the nature of our legal and compliance obligations. All Personal Information no longer required by us is destroyed and/or erased.\n\n## Personal Data from Third Parties\n\nWe may, from time to time, obtain Personal Information from third parties to enable us to better tailor our Services to you (Third Party Personal Information). When we obtain Third Party Personal Information, we will notify you within one month. If we use or share Third Party Personal Information, we will notify you immediately.\n\n## Your Rights\n\nWith respect to Personal Information we hold, you have the following rights:\n\n- Access: You may request from us access to your data that we hold on you.\n- Rectification: If the data we hold on you is inaccurate, you may request that we correct it. If the data we hold is incomplete, you may request that we complete it.\n- Erasure: Subject to certain conditions, you may request that we erase all of the data we hold on you.\n- Restrictions: Subject to certain conditions, you may request that we restrict the processing of data we hold on you.\n- Portability: Subject to certain conditions, you may request that we transfer all the data we hold on you to a third party (including yourself).\n- Objection: Subject to certain conditions, you may object to our processing of your data.\n\n## Minors\n\nWe do not knowingly collect information on children under the age of 16 (“Child”). If you become aware that a Child has provided us with Personal Information, please contact us. If we become aware that we have collected Personal Information from a Child without their parent's verifiable authorization to access our Services, we will take steps to remove that information from our servers.\n\n## Third Party Links\n\nOur website contains links to other third-party websites not owned or managed by Example LLC. This privacy policy applies to this website only. If you click a link to a third-party website, the privacy policy of that website will apply. We highly recommend that you read the privacy policies of other websites as they may be different from ours.\n\n## Merger, acquisition, or asset sale\n\nIf Example LLC is involved in a merger, acquisition, or asset sale, your Personal Information may be transferred.\n\n## Contact us\n\nIf you have any questions or suggestions about our privacy policy or want to know more information about Personal Information we hold, please contact us at [support@example.com](mailto:support@example.com).","src/data/otherPages/en/privacy-policy/index.mdx","7bb1a4eb05d5486e","blog",["Map",594,595,612,613],"en/why-split-dns-is-a-trap-and-you-shouldn-t-fall-for-it",{"id":594,"data":596,"body":607,"filePath":608,"assetImports":609,"digest":611,"deferredRender":23},{"title":597,"description":598,"authors":599,"pubDate":601,"heroImage":602,"categories":603,"draft":52},"Why Split DNS Is a Trap (and You Shouldn’t Fall for It)","A cautionary tale of hair-pulling debug sessions and invisible firewalls",[600],{"id":11,"collection":9},["Date","2025-04-24T00:00:00.000Z"],"__ASTRO_IMAGE_../why-split-dns-is-a-trap-and-you-shouldn-t-fall-for-it/heroImage.jpg",[604,159,605,606],"dns","Homelab","Networking","I recently had some serious troubles using split DNS in my home network. My prduction workloads are hybrid, some being in the cloud and some in local server rack. I thought I would be clever and implement split dns. Sounds harmless, right? Like maybe you're just _splitting_ up the responsibility nicely between your internal and external zones. It even sounds a little... organized?\n\nNah. It's a trap. A shiny, deliciously deceptive trap that seems like a good idea until it makes you question reality, DNS, and your life choices.\n\n---\n\n### What _Is_ Split DNS?\n\nSplit DNS is when you configure your internal DNS servers to resolve certain domains (usually internal names or internal versions of public services) differently than the public DNS.\n\nFor example:\n\n- Internal `api.example.com` → 10.10.1.42\n- External `api.example.com` → 203.0.113.42\n\nTo a human? Totally logical. To systems, automation tools, VPNs, and your future self? A recipe for tears.\n\n---\n\n### Why It’s a Bad Idea\n\n#### 1. **Inconsistent State, Inconsistent Bugs**\n\nEverything works fine… until it doesn’t. One laptop on the wrong network path, a recursive DNS server that leaks the wrong answer, or a VPN client that does something \"weird\" — suddenly you're troubleshooting a ghost.\n\n> \"It works for me.\"\\\n> \"Well, it doesn't work on my phone over LTE.\"\\\n> \"What DNS server are you using?\"\\\n> **Cue the spiral.**\n\n---\n\n#### 2. **Breaks Zero Trust and Modern Networking Models**\n\nSplit DNS assumes you _know_ who's inside and who's outside. That’s cute. But in 2025, we’ve got hybrid networks, Tailscale, cloud-hosted services, remote employees, and microservices talking across clusters.\n\nSplit DNS doesn’t fit. You want auth-based access, not “can this IP resolve the hostname?”\n\n---\n\n#### 3. **Certificate Misery**\n\nTrying to use Let’s Encrypt or any ACME-based cert issuance when your internal DNS points to an internal IP? Good luck. Your public resolver can’t hit it, and now you're proxying validation requests or running DNS challenges with duct tape.\n\nSSL termination becomes a hellscape of split zones and reverse proxy hacks. Every cert renewal is a ticking time bomb.\n\n---\n\n#### 4. **Caching Nightmares**\n\nThis was the worst nightmare of them all. DNS caching makes split DNS _extra spicy_. Your laptop switches from your office to a café, but it’s still got that internal IP cached. Now your curl command hangs, trying to reach an address that doesn’t exist outside your internal network.\n\nWanna debug that? Hope you love `dig`, `nslookup`, `systemd-resolved`, and crying, lots of crying.\n\n---\n\n### Okay, So What’s the Alternative?\n\n**Use a single, consistent DNS zone** and control access at the **network** or **application** level:\n\n- Use only public DNS entries, even for internal services. (Bad Idea)\n- Hard separations between internal and external services. For internal services internal-only subdomains (`internal.example.com`) instead of dual-purposing a public domain.\n- Embrace Tailscale or VPNs to securely expose internal services without DNS sleight-of-hand.\n\n---\n\n### TL;DR\n\nSplit DNS _seems_ like a good idea; until it becomes the source of untraceable bugs, inconsistent behavior, and broken cert automation. Don’t fall into the trap.\n\nIf you need different access levels for different users or locations, solve that with **auth**, not **alternate realities of DNS**.\n\nBecause the moment you’re deep into a split DNS debugging session at 2AM, you’ll understand why this post exists.","src/data/blog/en/why-split-dns-is-a-trap-and-you-shouldn-t-fall-for-it/index.mdx",[610],"../why-split-dns-is-a-trap-and-you-shouldn-t-fall-for-it/heroImage.jpg","f3a938d21e7b533b","en/k3s-raspberry-pi-the-perfect-diy-kubernetes-cluster",{"id":612,"data":614,"body":622,"filePath":623,"assetImports":624,"digest":626,"deferredRender":23},{"title":615,"description":616,"authors":617,"pubDate":619,"heroImage":620,"categories":621,"draft":52},"k3s and Raspberry Pi: The Perfect DIY Kubernetes Cluster","Run k3s Kubernetes on Raspberry Pi! This guide shows you how to build a lightweight cluster for home or homelab. Easy setup & powerful results.",[618],{"id":11,"collection":9},["Date","2025-06-17T00:00:00.000Z"],"__ASTRO_IMAGE_../k3s-raspberry-pi-the-perfect-diy-kubernetes-cluster/heroImage.png",[605,33,159],"### Kubernetes, But Make It Snack-Sized&#x20;\n\nI love Kubernetes, and I absolutely love pie. However, let's be real here, if you tried to run Kubernetes on a stack of raspberry pis, they would be more stuffed than me at thanksgiving dinner. Your poor little SBCs start sweating, and no way to loosen the belt.\n\nEnter k3s. The lightweight, certified Kubernetes distribution that’s so slim, it would make a fashion model feel inadequate. Designed by Rancher, k3s is just like Kubernetes: same power, way less bloat.\n\n### Why k3s Rocks My (and Your) Socks off\n\n- Tiny Footprint: k3s is a single binary under 100MB. It’s like Kubernetes, but on a diet.\n- Fast and Furious: Installs in under a minute, even on a Pi. You’ll spend more time making coffee.\n- Low Resource Usage: Perfect for ARM devices and low-power hardware. Your Pis will thank you.\n- Batteries Included: Comes with containerd, Flannel, Traefik, and more, but you can swap them out if you’re picky.\n- Certified K8s: It’s not a toy. k3s passes all CNCF conformance tests. You get the real deal.\n\nWhat Can You Do With a Pi Cluster Running k3s?\n\n- Home automation (Home Assistant, Node-RED, etc.)\n- Self-hosted services (Nextcloud, Gitea, Jellyfin, Immich)\n- Learning and experimentation (CI/CD, GitOps, edge computing)\n- Bragging rights at your next nerd meetup\n\n### How to Use k3s on a Raspberry Pi Cluster\n\nWhat You’ll Need\n\n- 2+ Raspberry Pis (any model works, but 4 or newer is best)\n- SD cards or USB to SSD (Recommended)\n- Network (wired is best, but WiFi works)\n- SSH access to each Pi\n- A little patience and a lot of coffee\n\nStep 1: Prep Your Pis\n\n1. Flash Raspberry Pi OS Lite (or your favorite flavor) onto each SD card.\n2. Enable SSH by dropping an empty file named `ssh` in the `/boot` partition.\n3. Boot up and SSH in to each Pi.\n4. Update everything (because you’re not a monster):\n\n   ```sh\n   sudo apt update && sudo apt upgrade -y\n   ```\n\n5. Set hostnames so you know who’s who:\n\n   ```sh\n   sudo hostnamectl set-hostname pi-master\n   # or pi-node1, pi-node2, etc.\n   ```\n\n6. Reboot for good measure.\n\nStep 2: Install k3s on the Master Node\n\nOn your “master” Pi (let’s call it `pi-master`):\n\n```sh\ncurl -sfL https://get.k3s.io | sh -\n```\n\nThis installs k3s and starts the server.\nGrab the node token for your agents:\n\n```sh\nsudo cat /var/lib/rancher/k3s/server/node-token\n```\n\nStep 3: Install k3s on the Worker Nodes\n\nOn each worker Pi:\n\n```sh\ncurl -sfL https://get.k3s.io | K3S_URL=https://\u003CMASTER_IP>:6443 K3S_TOKEN=\u003CNODE_TOKEN> sh -\n```\n\nReplace `\u003CMASTER_IP>` with your master’s IP.\nReplace `\u003CNODE_TOKEN>` with the token you copied.\n\nStep 4: Check Your Cluster\n\nBack on the master:\n\n```sh\nsudo k3s kubectl get nodes\n```\n\nYou should see all your Pis listed as happy, ready nodes. If not, check your network/firewall and make sure the token/IPs are correct.\n\nStep 5: Deploy Something Cool\n\nLet’s deploy a classic Nginx pod:\n\n```sh\nsudo k3s kubectl create deployment nginx --image=nginx\nsudo k3s kubectl expose deployment nginx --port=80 --type=NodePort\n```\n\nFind the NodePort:\n\n```sh\nsudo k3s kubectl get svc\n```\n\nNow hit your Pi’s IP at that port in your browser. Boom! You’re running Kubernetes on a cluster of $35 computers. You absolute legend.\n\n### Final Thoughts\n\nk3s makes Kubernetes accessible, fun, and practical for tinkerers, homelabbers, and edge deployments. Running it on a Raspberry Pi cluster is a fantastic way to learn, experiment, and build cool stuff without breaking the bank or your brain.\n\nSo go forth, cluster up, and may your pods always be healthy and your nodes never taint(ed).","src/data/blog/en/k3s-raspberry-pi-the-perfect-diy-kubernetes-cluster/index.mdx",[625],"../k3s-raspberry-pi-the-perfect-diy-kubernetes-cluster/heroImage.png","7fb25155d9dadbaf"]